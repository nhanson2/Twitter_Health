{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnltk.download(\\'punkt\\') -> missing tokenzer, might need installing\\nfor dependency in (\"words\", brown\", \"names\", \"wordnet\", \"averaged_perceptron_tagger\", \"universal_tagset\", \"stop_words\", \"maxent_ne_chunker\", \"punkt\"):\\n    nltk.download(dependency)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Statements\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import gzip\n",
    "import copy\n",
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "from afinn import Afinn\n",
    "from pathlib import Path\n",
    "from spacy import displacy\n",
    "from normalise import normalise\n",
    "from normalise import tokenize_basic\n",
    "from autocorrect import Speller\n",
    "from collections import Counter\n",
    "from gensim import corpora, models\n",
    "from nltk.tree import Tree\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "### LOAD CONFIGURATION LIBARIES\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_231/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "nlp = en_core_web_sm.load()\n",
    "st = StanfordNERTagger('C:/Users/NA29187/Downloads/stanford-ner-2018-10-16/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   'C:/Users/NA29187/Downloads/stanford-ner-2018-10-16/stanford-ner-2018-10-16/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\"\"\"\n",
    "nltk.download('punkt') -> missing tokenzer, might need installing\n",
    "for dependency in (\"words\", brown\", \"names\", \"wordnet\", \"averaged_perceptron_tagger\", \"universal_tagset\", \"stop_words\", \"maxent_ne_chunker\", \"punkt\"):\n",
    "    nltk.download(dependency)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangDict = {\n",
    "    'afaik': 'as far as i know',\n",
    "    'bc': 'because',\n",
    "    'bfn': 'bye for now',\n",
    "    'br': 'best regards',\n",
    "    'btw': 'by the way',\n",
    "    'dm': 'direct message',\n",
    "    'em': 'email',\n",
    "    'fb': 'facebook',\n",
    "    'ffs': 'for fucks sake',\n",
    "    'fml': 'fuck my life',\n",
    "    'rt': 'retweet',\n",
    "    'bgd': 'background',\n",
    "    'ab': 'about',\n",
    "    'abt': 'about',\n",
    "    'dd': 'dear daughter',\n",
    "    'ayfkmwts': 'are you fucking kidding me with this shit',\n",
    "    'w': 'with',\n",
    "    'wo': 'without',\n",
    "    'dyk': 'did you know',\n",
    "    'eli5': 'explain it to me like i am five',\n",
    "    'fbf': 'flashback friday',\n",
    "    'fomo': 'fear of missing out',\n",
    "    'ftw': 'for the win',\n",
    "    'fyi': 'for your information',\n",
    "    'icymi': 'in case you missed it',\n",
    "    'ht': 'hat tip',\n",
    "    'imo': 'in my opinion',\n",
    "    'imho': 'in my humble opinion',\n",
    "    'irl': 'in real life',\n",
    "    'lmk': 'let me know',\n",
    "    'nbd': 'no big deal',\n",
    "    'nsfw': 'not safe for work',\n",
    "    'wfh': 'working from home',\n",
    "    'covid': 'coronavirus',\n",
    "    'smh': 'shaking my head',\n",
    "    'tbh': 'to be honest',\n",
    "    'tbt': 'throwback thursday',\n",
    "    'tfw': 'that feeling when',\n",
    "    'tgif': 'thank god it is friday',\n",
    "    'tldr': 'too long did not read',\n",
    "    'wbw': 'way back wednesday',\n",
    "    'af': 'as fuck',\n",
    "    'bae': 'my crush',\n",
    "    'hmu': 'hit me up',\n",
    "    'idk': 'i do not know',\n",
    "    'idc': 'i do not care',\n",
    "    'fwiw': 'for what it is worth',\n",
    "    'ily': 'i love you',\n",
    "    'iso': 'in search of',\n",
    "    'jk': 'just kidding',\n",
    "    'jtm': 'just the messenger',\n",
    "    'lmao': 'laughing my ass off',\n",
    "    'lol': 'laughing out loud',\n",
    "    'rip': 'rest in peace',\n",
    "    'nvm': 'nevermind',\n",
    "    'obv': 'obviously',\n",
    "    'omg': 'oh my god',\n",
    "    'omw': 'on my way',\n",
    "    'pls': 'please',\n",
    "    'psa': 'public service announcement',\n",
    "    'rn': 'right now',\n",
    "    'rofl': 'rolling on the floor laughing',\n",
    "    'srsly': 'seriously',\n",
    "    'til': 'today i learned',\n",
    "    'tmi': 'too much information',\n",
    "    'ty': 'thank you',\n",
    "    'wtf': 'what the fuck',\n",
    "    'yw': 'you are welcome',\n",
    "    'bfn': 'bye for now',\n",
    "    'da': 'the',\n",
    "    'deets': 'details',\n",
    "    'fab': 'fabulous',\n",
    "    'fav': 'favorite',\n",
    "    'ic': 'i see',\n",
    "    'kk': 'cool cool',\n",
    "    'nts': 'note to self',\n",
    "    'prt': 'please retweet',\n",
    "    'tftf': 'thank for the follow',\n",
    "    'tmb': 'tweet me back',\n",
    "    'u': 'you',\n",
    "    'woz': 'was',\n",
    "    'wtv': 'whatever',\n",
    "    'ykyat': 'you know you are addicted to',\n",
    "    'yolo': 'you only live once',\n",
    "    'yoyo': 'you are on your own'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(maxNum=1000):\n",
    "    ''' Utilize the get old tweets package to aggregate historical tweets - easier to use than Twitter API '''\n",
    "    # Build the OS call to generate the csv\n",
    "    FILE_PATH = 'C:/Users/NA29187/Downloads/Optimized-Modified-GetOldTweets3-OMGOT-master/Optimized-Modified-GetOldTweets3-OMGOT-master/GetOldTweets3-0.0.10/'\n",
    "    SCRIPT = 'GetOldTweets3.py '\n",
    "    PYTHON_PATH = 'C:/ProgramData/Anaconda3/python.exe '\n",
    "    QUERY = '--near \"42.376, -71.1097\" --within 40km --maxtweets {}'.format(maxNum)\n",
    "    print('Executing Query: {}'.format(PYTHON_PATH + FILE_PATH + SCRIPT + QUERY))\n",
    "    os.system(PYTHON_PATH + FILE_PATH + SCRIPT + QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Query: C:/ProgramData/Anaconda3/python.exe C:/Users/NA29187/Downloads/Optimized-Modified-GetOldTweets3-OMGOT-master/Optimized-Modified-GetOldTweets3-OMGOT-master/GetOldTweets3-0.0.10/GetOldTweets3.py --near \"42.376, -71.1097\" --within 40km --maxtweets 500\n"
     ]
    }
   ],
   "source": [
    "generate_corpus(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    ''' Read in downloaded tweet meta data and extract key fields '''\n",
    "    data = pd.read_csv(\"output_got.csv\")\n",
    "    # Remove useless columns to save space in memory\n",
    "    del data['replies']\n",
    "    del data['retweets']\n",
    "    del data['favorites']\n",
    "    del data['hashtags']\n",
    "    del data['permalink']\n",
    "    # Create area to store cleaned tweet\n",
    "    data['cleaned_tweet'] = data['text']\n",
    "    data['tokens'] = ''\n",
    "    data['parts_of_speech'] = ''\n",
    "    data['chunks'] = ''\n",
    "    data['entities'] = ''\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweet']= data['cleaned_tweet'].apply(clean_text)\n",
    "data['tokens'] = data['cleaned_tweet'].apply(token_pipeline)\n",
    "data['parts_of_speech'] = data['tokens'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "      <th>geo</th>\n",
       "      <th>mentions</th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>parts_of_speech</th>\n",
       "      <th>chunks</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-29 13:41:58</td>\n",
       "      <td>bigsnugs16</td>\n",
       "      <td>thomaskaine5</td>\n",
       "      <td>Oh yes. They are so bad. We need left wing med...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492524556369920</td>\n",
       "      <td>oh yes they are so bad we need left wing media...</td>\n",
       "      <td>[oh, yes, bad, need, left, wing, medium, know,...</td>\n",
       "      <td>[(oh, UH), (yes, RB), (bad, JJ), (need, NN), (...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-29 13:41:58</td>\n",
       "      <td>Pythagasaurus2</td>\n",
       "      <td>matthewstoller</td>\n",
       "      <td>He'd be a real improvement though. Actual prin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492522840915968</td>\n",
       "      <td>hed be a real improvement though actual princi...</td>\n",
       "      <td>[hed, real, improvement, though, actual, princ...</td>\n",
       "      <td>[(hed, VBN), (real, JJ), (improvement, NN), (t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-29 13:41:57</td>\n",
       "      <td>barbarapdavis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>During coronavirus pandemic, some Massachusett...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492522723467266</td>\n",
       "      <td>during coronavirus pandemic some massachusetts...</td>\n",
       "      <td>[coronavirus, pandemic, massachusetts, high, s...</td>\n",
       "      <td>[(coronavirus, NN), (pandemic, JJ), (massachus...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-29 13:41:57</td>\n",
       "      <td>CharizardLaw</td>\n",
       "      <td>tylenlampkin</td>\n",
       "      <td>I *****need***** these</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492522358472707</td>\n",
       "      <td>i need these</td>\n",
       "      <td>[need]</td>\n",
       "      <td>[(need, NN)]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-29 13:41:57</td>\n",
       "      <td>equus_boston</td>\n",
       "      <td>WSJ</td>\n",
       "      <td>こんな時でも株価は上昇⁈https://twitter.com/wsj/status/125...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492522144645121</td>\n",
       "      <td>httpstwittercomwsjstatus1255492186604556290</td>\n",
       "      <td>[httpstwittercomwsjstatus, 12554921866, 04556290]</td>\n",
       "      <td>[(httpstwittercomwsjstatus, NN), (12554921866,...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-04-29 13:41:57</td>\n",
       "      <td>jww372</td>\n",
       "      <td>yashar</td>\n",
       "      <td>Keep hope alive.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492522052325387</td>\n",
       "      <td>keep hope alive</td>\n",
       "      <td>[keep, hope, alive]</td>\n",
       "      <td>[(keep, VB), (hope, NN), (alive, JJ)]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-04-29 13:41:57</td>\n",
       "      <td>lesegokgaladi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Looks like you have to order essentials in dou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492521884618767</td>\n",
       "      <td>looks like you have to order essentials in dou...</td>\n",
       "      <td>[look, like, order, essential, double, one, ar...</td>\n",
       "      <td>[(look, NN), (like, IN), (order, NN), (essenti...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-04-29 13:41:56</td>\n",
       "      <td>in4ins</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Revisiting agile teams after an abrupt shift t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@McKinsey</td>\n",
       "      <td>1255492517518290946</td>\n",
       "      <td>revisiting agile teams after an abrupt shift t...</td>\n",
       "      <td>[revisiting, agile, team, abrupt, shift, remot...</td>\n",
       "      <td>[(revisiting, VBG), (agile, JJ), (team, NN), (...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-04-29 13:41:55</td>\n",
       "      <td>stephanbruh</td>\n",
       "      <td>SnewHef</td>\n",
       "      <td>he got strapped by the mavs defense, they play...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492510471856132</td>\n",
       "      <td>he got strapped by the mavs defense they playe...</td>\n",
       "      <td>[got, strapped, mass, defense, played, zone, l...</td>\n",
       "      <td>[(got, VBD), (strapped, JJ), (mass, NN), (defe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-04-29 13:41:55</td>\n",
       "      <td>JimClinkscale</td>\n",
       "      <td>MMMac88</td>\n",
       "      <td>Is there a fee difference for flaccid vs. erect?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492510346096648</td>\n",
       "      <td>is there a fee difference for flaccid vs erect</td>\n",
       "      <td>[fee, difference, flaccid, v, erect]</td>\n",
       "      <td>[(fee, NN), (difference, NN), (flaccid, NN), (...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-04-29 13:41:54</td>\n",
       "      <td>metamitya</td>\n",
       "      <td>steveschoger</td>\n",
       "      <td>https://twitter.com/metamitya/status/124655593...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492508668301316</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-04-29 13:41:54</td>\n",
       "      <td>bobricebobrice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>having a great morning! i've listened to PUP's...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492508462825472</td>\n",
       "      <td>having a great morning ive listened to pups ne...</td>\n",
       "      <td>[great, morning, ive, listened, pup, new, song...</td>\n",
       "      <td>[(great, JJ), (morning, NN), (ive, NN), (liste...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-04-29 13:41:54</td>\n",
       "      <td>xTanaidi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m TIRED of smoking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492508324442114</td>\n",
       "      <td>im tired of smoking</td>\n",
       "      <td>[im, tired, smoking]</td>\n",
       "      <td>[(im, NN), (tired, VBD), (smoking, NN)]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-04-29 13:41:54</td>\n",
       "      <td>J0eyCasco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The cribbo #AnimalCrossing #ACNH #NintendoSwit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492506931822592</td>\n",
       "      <td>the cribbo animalcrossing acnh nintendoswitchp...</td>\n",
       "      <td>[cribo, animalcrossing, acne, nintendoswitchpi...</td>\n",
       "      <td>[(cribo, NN), (animalcrossing, VBG), (acne, JJ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-04-29 13:41:53</td>\n",
       "      <td>Ian_W_Evans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apparently @CenterForBioDiv has a \"Population ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@CenterForBioDiv @CenterForBioDiv</td>\n",
       "      <td>1255492505015115777</td>\n",
       "      <td>apparently centerforbiodiv has a population an...</td>\n",
       "      <td>[apparently, centerforbiodiv, population, sust...</td>\n",
       "      <td>[(apparently, RB), (centerforbiodiv, JJ), (pop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-04-29 13:41:52</td>\n",
       "      <td>CharlesNicess</td>\n",
       "      <td>AllyMcBands</td>\n",
       "      <td>Ohhh uh uh What he do ?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492500187463680</td>\n",
       "      <td>ohhh uh uh what he do</td>\n",
       "      <td>[oath, uh, uh]</td>\n",
       "      <td>[(oath, NN), (uh, NN), (uh, NN)]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-04-29 13:41:50</td>\n",
       "      <td>thelostemperor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This tweet is in response to Dems falling all ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492489559052291</td>\n",
       "      <td>this tweet is in response to dems falling all ...</td>\n",
       "      <td>[tweet, response, gem, falling, scold, pilosis...</td>\n",
       "      <td>[(tweet, NN), (response, NN), (gem, NN), (fall...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-04-29 13:41:49</td>\n",
       "      <td>KyleSGibson</td>\n",
       "      <td>KyleSGibson</td>\n",
       "      <td>A mere gut feeling that Tether is receiving bi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492488380514306</td>\n",
       "      <td>a mere gut feeling that tether is receiving bi...</td>\n",
       "      <td>[mere, gut, feeling, tether, receiving, billio...</td>\n",
       "      <td>[(mere, RB), (gut, JJ), (feeling, VBG), (tethe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-04-29 13:41:48</td>\n",
       "      <td>smutlover89</td>\n",
       "      <td>BrokenGamezHDR_</td>\n",
       "      <td>Tell that nigga kid smoove to sit down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492484697899010</td>\n",
       "      <td>tell that nigga kid smoove to sit down</td>\n",
       "      <td>[tell, nigua, kid, snoove, sit]</td>\n",
       "      <td>[(tell, VB), (nigua, JJ), (kid, NN), (snoove, ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-04-29 13:41:48</td>\n",
       "      <td>yT_BME</td>\n",
       "      <td>hirosetakao</td>\n",
       "      <td>いずれできるであろうワクチンもそうなるでしょうね。まず自国を優先するでしょうから。https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492483959701505</td>\n",
       "      <td>httpstwittercomhirosetakaostatus12554862668351...</td>\n",
       "      <td>[httpstwittercomhirosetakaostatus, 12554862668...</td>\n",
       "      <td>[(httpstwittercomhirosetakaostatus, NN), (1255...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-04-29 13:41:48</td>\n",
       "      <td>JaymeZontini</td>\n",
       "      <td>T_S_P_O_O_K_Y</td>\n",
       "      <td>What has been bothering me this entire time is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492482617442307</td>\n",
       "      <td>what has been bothering me this entire time is...</td>\n",
       "      <td>[bothering, entire, time, quidproquo, actually...</td>\n",
       "      <td>[(bothering, VBG), (entire, JJ), (time, NN), (...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-04-29 13:41:47</td>\n",
       "      <td>so_l___</td>\n",
       "      <td>Luigi6132</td>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492479543017477</td>\n",
       "      <td>b</td>\n",
       "      <td>[b]</td>\n",
       "      <td>[(b, NN)]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020-04-29 13:41:47</td>\n",
       "      <td>Intxrcxpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just another day to improve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492479526342656</td>\n",
       "      <td>just another day to improve</td>\n",
       "      <td>[another, day, improve]</td>\n",
       "      <td>[(another, DT), (day, NN), (improve, VB)]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-04-29 13:41:47</td>\n",
       "      <td>BostonFireAlert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Engine 42 Respiratory Distress. 71 Heath Stree...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492478062362624</td>\n",
       "      <td>engine 42 respiratory distress 71 heath street...</td>\n",
       "      <td>[engine, 42, respiratory, distress, 71, heath,...</td>\n",
       "      <td>[(engine, NN), (42, CD), (respiratory, NN), (d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-04-29 13:41:47</td>\n",
       "      <td>CathrynMcIntyre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>''Just so hollow and ineffectual, for the most...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492476821024783</td>\n",
       "      <td>just so hollow and ineffectual for the most pa...</td>\n",
       "      <td>[hollow, ineffectual, part, ordinary, conversa...</td>\n",
       "      <td>[(hollow, JJ), (ineffectual, JJ), (part, NN), ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020-04-29 13:41:46</td>\n",
       "      <td>mollymmcginty</td>\n",
       "      <td>WCYA2020</td>\n",
       "      <td>Hi Salisbury friends! I don’t get on this acco...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492475193622539</td>\n",
       "      <td>hi salisbury friends i dont get on this accoun...</td>\n",
       "      <td>[hi, salisbury, friend, dont, get, account, mu...</td>\n",
       "      <td>[(hi, JJ), (salisbury, NN), (friend, NN), (don...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2020-04-29 13:41:46</td>\n",
       "      <td>DtheInformed</td>\n",
       "      <td>MaryWCVB</td>\n",
       "      <td>Shouldn't he be fined for not wearing a mask?h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492474300137472</td>\n",
       "      <td>shouldnt he be fined for not wearing a maskhtt...</td>\n",
       "      <td>[shouldnt, fined, wearing, maskhttpstwittercom...</td>\n",
       "      <td>[(shouldnt, NN), (fined, VBD), (wearing, VBG),...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-04-29 13:41:46</td>\n",
       "      <td>BrainsLink</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COVID Symptom Tracker - Help slow the spread o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492472454688771</td>\n",
       "      <td>covid symptom tracker  help slow the spread of...</td>\n",
       "      <td>[coronavirus, symptom, tracker, help, slow, sp...</td>\n",
       "      <td>[(coronavirus, NN), (symptom, NN), (tracker, N...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-04-29 13:41:45</td>\n",
       "      <td>lifeinbible</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Today (Apr 29) DAY 29: PRAYER FOR THE PREPARAT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492472207282176</td>\n",
       "      <td>today apr 29 day 29 prayer for the preparation...</td>\n",
       "      <td>[today, apr, 29, day, 29, prayer, preparation,...</td>\n",
       "      <td>[(today, NN), (apr, VBD), (29, CD), (day, NN),...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2020-04-29 13:41:44</td>\n",
       "      <td>newbury_eric</td>\n",
       "      <td>JustinWolfers</td>\n",
       "      <td>Which is ironic given that the hordes of red r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255492467832602625</td>\n",
       "      <td>which is ironic given that the hordes of red r...</td>\n",
       "      <td>[ironic, given, horde, red, rose, democratic, ...</td>\n",
       "      <td>[(ironic, JJ), (given, VBN), (horde, NN), (red...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>2020-04-29 13:35:08</td>\n",
       "      <td>Thriveworks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>During times like these, it's important to sta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490806288461825</td>\n",
       "      <td>during times like these its important to stay ...</td>\n",
       "      <td>[time, like, important, stay, connected, frien...</td>\n",
       "      <td>[(time, NN), (like, IN), (important, JJ), (sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>2020-04-29 13:35:08</td>\n",
       "      <td>Jonmicol</td>\n",
       "      <td>MannieeGeee</td>\n",
       "      <td>If i made u my kind of tostones u would fall i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490804870713349</td>\n",
       "      <td>if i made u my kind of tostones u would fall i...</td>\n",
       "      <td>[made, you, kind, topstones, you, would, fall,...</td>\n",
       "      <td>[(made, VBN), (you, PRP), (kind, NN), (topston...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>2020-04-29 13:35:08</td>\n",
       "      <td>ReadingBxHealth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How to Stop Feeling So Helpless During Quarant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490803834773509</td>\n",
       "      <td>how to stop feeling so helpless during quarant...</td>\n",
       "      <td>[stop, feeling, helpless, quarantinehttpsgreat...</td>\n",
       "      <td>[(stop, VB), (feeling, NN), (helpless, JJ), (q...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>2020-04-29 13:35:08</td>\n",
       "      <td>AustinPrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our families continue to spend quality time to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490803117555713</td>\n",
       "      <td>our families continue to spend quality time to...</td>\n",
       "      <td>[family, continue, spend, quality, time, toget...</td>\n",
       "      <td>[(family, NN), (continue, VBP), (spend, VBP), ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>2020-04-29 13:35:06</td>\n",
       "      <td>renieplayerone</td>\n",
       "      <td>renieplayerone</td>\n",
       "      <td>How would you like it if someone came up to yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490796066930688</td>\n",
       "      <td>how would you like it if someone came up to yo...</td>\n",
       "      <td>[would, like, someone, came, told, arent, vali...</td>\n",
       "      <td>[(would, MD), (like, VB), (someone, NN), (came...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>2020-04-29 13:35:05</td>\n",
       "      <td>mangarosabetim</td>\n",
       "      <td>UOLEsporte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490794506661890</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>2020-04-29 13:35:05</td>\n",
       "      <td>NESN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bruce Cassidy shared his connection to Bobby O...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490791717437443</td>\n",
       "      <td>bruce cassidy shared his connection to bobby o...</td>\n",
       "      <td>[bruce, cassady, shared, connection, bobby, ou...</td>\n",
       "      <td>[(bruce, NN), (cassady, NN), (shared, VBN), (c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>2020-04-29 13:35:04</td>\n",
       "      <td>LaydeeTraceY</td>\n",
       "      <td>just1ak</td>\n",
       "      <td>It stains my Tupperware but I think I just nee...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490789968404480</td>\n",
       "      <td>it stains my tupperware but i think i just nee...</td>\n",
       "      <td>[stain, supperward, think, need, get, get, bac...</td>\n",
       "      <td>[(stain, RB), (supperward, JJ), (think, VBP), ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>2020-04-29 13:35:04</td>\n",
       "      <td>Brewbound</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Empourium to Release A Plethora Of Piñatas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490789465100303</td>\n",
       "      <td>the empourium to release a plethora of piatas ...</td>\n",
       "      <td>[emporium, release, plethora, riata, mexican, ...</td>\n",
       "      <td>[(emporium, NN), (release, NN), (plethora, NN)...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>2020-04-29 13:35:04</td>\n",
       "      <td>CellPressNews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancer Cells Resist Mechanical Destruction in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@CellReports</td>\n",
       "      <td>1255490788194234370</td>\n",
       "      <td>cancer cells resist mechanical destruction in ...</td>\n",
       "      <td>[cancer, cell, resist, mechanical, destruction...</td>\n",
       "      <td>[(cancer, NN), (cell, NN), (resist, VB), (mech...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>2020-04-29 13:35:04</td>\n",
       "      <td>mapocoloco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dow surges more than 400 points as positive Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@CNBChttps</td>\n",
       "      <td>1255490787573456906</td>\n",
       "      <td>dow surges more than 400 points as positive gi...</td>\n",
       "      <td>[dow, surge, 400, point, positive, glead, news...</td>\n",
       "      <td>[(dow, NN), (surge, NN), (400, CD), (point, NN...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>2020-04-29 13:35:04</td>\n",
       "      <td>TylerJKelemen</td>\n",
       "      <td>harrysflorals</td>\n",
       "      <td>Dude. Is that Miss Trunchbull?! https://twitte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490786956914688</td>\n",
       "      <td>dude is that miss trunchbull httpstwittercomha...</td>\n",
       "      <td>[dude, miss, trunchbull, httpstwittercomharrys...</td>\n",
       "      <td>[(dude, NN), (miss, CC), (trunchbull, NN), (ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>2020-04-29 13:35:04</td>\n",
       "      <td>Eduporium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ignite #classroom-wide inventing and add a rea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490786751365125</td>\n",
       "      <td>ignite classroomwide inventing and add a realw...</td>\n",
       "      <td>[ignite, classroomwide, inventing, add, dreamw...</td>\n",
       "      <td>[(ignite, JJ), (classroomwide, NN), (inventing...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>2020-04-29 13:35:04</td>\n",
       "      <td>EducationNext</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A @WSJ editorial warns: “The pandemic will pas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@WSJ</td>\n",
       "      <td>1255490786403266560</td>\n",
       "      <td>a wsj editorial warns the pandemic will pass b...</td>\n",
       "      <td>[wsw, editorial, warns, pandemic, pas, used, c...</td>\n",
       "      <td>[(wsw, JJ), (editorial, NN), (warns, NNS), (pa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>2020-04-29 13:35:03</td>\n",
       "      <td>ScarletMagdalen</td>\n",
       "      <td>DiamondGlyph</td>\n",
       "      <td>Good news is that he's likely to take votes aw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490785732108288</td>\n",
       "      <td>good news is that hes likely to take votes awa...</td>\n",
       "      <td>[good, news, he, likely, take, vote, away, trump]</td>\n",
       "      <td>[(good, JJ), (news, NN), (he, PRP), (likely, R...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>2020-04-29 13:35:03</td>\n",
       "      <td>TweetBrettMac</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just found out what a SWERF is. I'll save you ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490785556008962</td>\n",
       "      <td>just found out what a swerf is ill save you th...</td>\n",
       "      <td>[found, swerf, ill, save, time, woman, basic, ...</td>\n",
       "      <td>[(found, VBN), (swerf, NN), (ill, NN), (save, ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>2020-04-29 13:35:03</td>\n",
       "      <td>sjpsnh</td>\n",
       "      <td>AuthorKimberley</td>\n",
       "      <td>Hard to find someone.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490784855539713</td>\n",
       "      <td>hard to find someone</td>\n",
       "      <td>[hard, find, someone]</td>\n",
       "      <td>[(hard, JJ), (find, VBP), (someone, NN)]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>2020-04-29 13:35:03</td>\n",
       "      <td>IcyTatumSZN</td>\n",
       "      <td>FSKfanclub</td>\n",
       "      <td>Bro you keep replying let it go</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490783832129540</td>\n",
       "      <td>bro you keep replying let it go</td>\n",
       "      <td>[bro, keep, replying, let, go]</td>\n",
       "      <td>[(bro, NN), (keep, VB), (replying, VBG), (let,...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>2020-04-29 13:35:02</td>\n",
       "      <td>mspardini</td>\n",
       "      <td>pacifygallagher</td>\n",
       "      <td>And a time machine, along with the ability to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490781906903041</td>\n",
       "      <td>and a time machine along with the ability to s...</td>\n",
       "      <td>[time, machine, along, ability, seduce, succes...</td>\n",
       "      <td>[(time, NN), (machine, NN), (along, IN), (abil...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>2020-04-29 13:35:02</td>\n",
       "      <td>SmarterTravel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“If you had to get married in a chain restaura...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490780338311170</td>\n",
       "      <td>if you had to get married in a chain restauran...</td>\n",
       "      <td>[get, married, chain, restaurant, one, would, ...</td>\n",
       "      <td>[(get, VB), (married, VBN), (chain, NN), (rest...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>2020-04-29 13:35:02</td>\n",
       "      <td>glowingniight</td>\n",
       "      <td>taeyeonhub</td>\n",
       "      <td>Lmao this is after moonie made me change it - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490779650437125</td>\n",
       "      <td>lmao this is after moonie made me change it  i...</td>\n",
       "      <td>[laughing my ass off, moonie, made, change, on...</td>\n",
       "      <td>[(laughing my ass off, NN), (moonie, NN), (mad...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>2020-04-29 13:35:02</td>\n",
       "      <td>CambridgeArts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>. @Cambridge_Side is looking for local artists...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Cambridge_Side</td>\n",
       "      <td>1255490778933190657</td>\n",
       "      <td>cambridgeside is looking for local artists to ...</td>\n",
       "      <td>[cambridgeside, looking, local, artist, featur...</td>\n",
       "      <td>[(cambridgeside, NN), (looking, VBG), (local, ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>2020-04-29 13:35:01</td>\n",
       "      <td>VestigoVentures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Faced with a new generation of online threats,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@DeepInstinctSec</td>\n",
       "      <td>1255490777628753921</td>\n",
       "      <td>faced with a new generation of online threats ...</td>\n",
       "      <td>[faced, new, generation, online, threat, smart...</td>\n",
       "      <td>[(faced, VBN), (new, JJ), (generation, NN), (o...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>2020-04-29 13:35:01</td>\n",
       "      <td>EmmaGZRoberts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11 years of shouting into a void :) I was a ju...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490777628708866</td>\n",
       "      <td>11 years of shouting into a void  i was a juni...</td>\n",
       "      <td>[11, year, shouting, void, junior, high, schoo...</td>\n",
       "      <td>[(11, CD), (year, NN), (shouting, VBG), (void,...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>2020-04-29 13:35:00</td>\n",
       "      <td>tplumitallo</td>\n",
       "      <td>AllyParente</td>\n",
       "      <td>...... I’m tearing up for you, I really am.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490772952023041</td>\n",
       "      <td>im tearing up for you i really am</td>\n",
       "      <td>[im, tearing, really]</td>\n",
       "      <td>[(im, NN), (tearing, VBG), (really, RB)]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2020-04-29 13:35:00</td>\n",
       "      <td>eMediaJunction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alexa B. Kimball, MD, MPH had a great conversa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Matt_R_Fisher</td>\n",
       "      <td>1255490772507525123</td>\n",
       "      <td>alexa b kimball md mph had a great conversatio...</td>\n",
       "      <td>[alexia, b, simball, md, mph, great, conversat...</td>\n",
       "      <td>[(alexia, NN), (b, NN), (simball, NN), (md, NN...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2020-04-29 13:35:00</td>\n",
       "      <td>3AWear</td>\n",
       "      <td>Jeffboots</td>\n",
       "      <td>https://youtu.be/LbXq_Zn5QFg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490771500912654</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2020-04-29 13:35:00</td>\n",
       "      <td>moma_tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Follow along as Thomas Carter surprises the te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490771064676353</td>\n",
       "      <td>follow along as thomas carter surprises the te...</td>\n",
       "      <td>[follow, along, thomas, carter, surprise, team...</td>\n",
       "      <td>[(follow, VB), (along, IN), (thomas, NN), (car...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2020-04-29 13:35:00</td>\n",
       "      <td>wbznewsradio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adams Square Baptist Church Pastor Kristopher ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490770578137090</td>\n",
       "      <td>adams square baptist church pastor kristopher ...</td>\n",
       "      <td>[adam, square, baptist, church, pastor, christ...</td>\n",
       "      <td>[(adam, NNS), (square, JJ), (baptist, JJ), (ch...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2020-04-29 13:35:00</td>\n",
       "      <td>adamfeuerstein</td>\n",
       "      <td>adamfeuerstein</td>\n",
       "      <td>Matt and I updated our remdesivir story with m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1255490770100015109</td>\n",
       "      <td>matt and i updated our remdesivir story with m...</td>\n",
       "      <td>[matt, updated, remdesivir, story, information...</td>\n",
       "      <td>[(matt, NN), (updated, VBD), (remdesivir, JJ),...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date         username               to  \\\n",
       "0    2020-04-29 13:41:58       bigsnugs16     thomaskaine5   \n",
       "1    2020-04-29 13:41:58   Pythagasaurus2   matthewstoller   \n",
       "2    2020-04-29 13:41:57    barbarapdavis              NaN   \n",
       "3    2020-04-29 13:41:57     CharizardLaw     tylenlampkin   \n",
       "4    2020-04-29 13:41:57     equus_boston              WSJ   \n",
       "5    2020-04-29 13:41:57           jww372           yashar   \n",
       "6    2020-04-29 13:41:57    lesegokgaladi              NaN   \n",
       "7    2020-04-29 13:41:56           in4ins              NaN   \n",
       "8    2020-04-29 13:41:55      stephanbruh          SnewHef   \n",
       "9    2020-04-29 13:41:55    JimClinkscale          MMMac88   \n",
       "10   2020-04-29 13:41:54        metamitya     steveschoger   \n",
       "11   2020-04-29 13:41:54   bobricebobrice              NaN   \n",
       "12   2020-04-29 13:41:54         xTanaidi              NaN   \n",
       "13   2020-04-29 13:41:54        J0eyCasco              NaN   \n",
       "14   2020-04-29 13:41:53      Ian_W_Evans              NaN   \n",
       "15   2020-04-29 13:41:52    CharlesNicess      AllyMcBands   \n",
       "16   2020-04-29 13:41:50   thelostemperor              NaN   \n",
       "17   2020-04-29 13:41:49      KyleSGibson      KyleSGibson   \n",
       "18   2020-04-29 13:41:48      smutlover89  BrokenGamezHDR_   \n",
       "19   2020-04-29 13:41:48           yT_BME      hirosetakao   \n",
       "20   2020-04-29 13:41:48     JaymeZontini    T_S_P_O_O_K_Y   \n",
       "21   2020-04-29 13:41:47          so_l___        Luigi6132   \n",
       "22   2020-04-29 13:41:47        Intxrcxpt              NaN   \n",
       "23   2020-04-29 13:41:47  BostonFireAlert              NaN   \n",
       "24   2020-04-29 13:41:47  CathrynMcIntyre              NaN   \n",
       "25   2020-04-29 13:41:46    mollymmcginty         WCYA2020   \n",
       "26   2020-04-29 13:41:46     DtheInformed         MaryWCVB   \n",
       "27   2020-04-29 13:41:46       BrainsLink              NaN   \n",
       "28   2020-04-29 13:41:45      lifeinbible              NaN   \n",
       "29   2020-04-29 13:41:44     newbury_eric    JustinWolfers   \n",
       "..                   ...              ...              ...   \n",
       "970  2020-04-29 13:35:08      Thriveworks              NaN   \n",
       "971  2020-04-29 13:35:08         Jonmicol      MannieeGeee   \n",
       "972  2020-04-29 13:35:08  ReadingBxHealth              NaN   \n",
       "973  2020-04-29 13:35:08       AustinPrep              NaN   \n",
       "974  2020-04-29 13:35:06   renieplayerone   renieplayerone   \n",
       "975  2020-04-29 13:35:05   mangarosabetim       UOLEsporte   \n",
       "976  2020-04-29 13:35:05             NESN              NaN   \n",
       "977  2020-04-29 13:35:04     LaydeeTraceY          just1ak   \n",
       "978  2020-04-29 13:35:04        Brewbound              NaN   \n",
       "979  2020-04-29 13:35:04    CellPressNews              NaN   \n",
       "980  2020-04-29 13:35:04       mapocoloco              NaN   \n",
       "981  2020-04-29 13:35:04    TylerJKelemen    harrysflorals   \n",
       "982  2020-04-29 13:35:04        Eduporium              NaN   \n",
       "983  2020-04-29 13:35:04    EducationNext              NaN   \n",
       "984  2020-04-29 13:35:03  ScarletMagdalen     DiamondGlyph   \n",
       "985  2020-04-29 13:35:03    TweetBrettMac              NaN   \n",
       "986  2020-04-29 13:35:03           sjpsnh  AuthorKimberley   \n",
       "987  2020-04-29 13:35:03      IcyTatumSZN       FSKfanclub   \n",
       "988  2020-04-29 13:35:02        mspardini  pacifygallagher   \n",
       "989  2020-04-29 13:35:02    SmarterTravel              NaN   \n",
       "990  2020-04-29 13:35:02    glowingniight       taeyeonhub   \n",
       "991  2020-04-29 13:35:02    CambridgeArts              NaN   \n",
       "992  2020-04-29 13:35:01  VestigoVentures              NaN   \n",
       "993  2020-04-29 13:35:01    EmmaGZRoberts              NaN   \n",
       "994  2020-04-29 13:35:00      tplumitallo      AllyParente   \n",
       "995  2020-04-29 13:35:00   eMediaJunction              NaN   \n",
       "996  2020-04-29 13:35:00           3AWear        Jeffboots   \n",
       "997  2020-04-29 13:35:00          moma_tx              NaN   \n",
       "998  2020-04-29 13:35:00     wbznewsradio              NaN   \n",
       "999  2020-04-29 13:35:00   adamfeuerstein   adamfeuerstein   \n",
       "\n",
       "                                                  text  geo  \\\n",
       "0    Oh yes. They are so bad. We need left wing med...  NaN   \n",
       "1    He'd be a real improvement though. Actual prin...  NaN   \n",
       "2    During coronavirus pandemic, some Massachusett...  NaN   \n",
       "3                               I *****need***** these  NaN   \n",
       "4    こんな時でも株価は上昇⁈https://twitter.com/wsj/status/125...  NaN   \n",
       "5                                     Keep hope alive.  NaN   \n",
       "6    Looks like you have to order essentials in dou...  NaN   \n",
       "7    Revisiting agile teams after an abrupt shift t...  NaN   \n",
       "8    he got strapped by the mavs defense, they play...  NaN   \n",
       "9     Is there a fee difference for flaccid vs. erect?  NaN   \n",
       "10   https://twitter.com/metamitya/status/124655593...  NaN   \n",
       "11   having a great morning! i've listened to PUP's...  NaN   \n",
       "12                                I’m TIRED of smoking  NaN   \n",
       "13   The cribbo #AnimalCrossing #ACNH #NintendoSwit...  NaN   \n",
       "14   Apparently @CenterForBioDiv has a \"Population ...  NaN   \n",
       "15                             Ohhh uh uh What he do ?  NaN   \n",
       "16   This tweet is in response to Dems falling all ...  NaN   \n",
       "17   A mere gut feeling that Tether is receiving bi...  NaN   \n",
       "18              Tell that nigga kid smoove to sit down  NaN   \n",
       "19   いずれできるであろうワクチンもそうなるでしょうね。まず自国を優先するでしょうから。https...  NaN   \n",
       "20   What has been bothering me this entire time is...  NaN   \n",
       "21                                                   b  NaN   \n",
       "22                         Just another day to improve  NaN   \n",
       "23   Engine 42 Respiratory Distress. 71 Heath Stree...  NaN   \n",
       "24   ''Just so hollow and ineffectual, for the most...  NaN   \n",
       "25   Hi Salisbury friends! I don’t get on this acco...  NaN   \n",
       "26   Shouldn't he be fined for not wearing a mask?h...  NaN   \n",
       "27   COVID Symptom Tracker - Help slow the spread o...  NaN   \n",
       "28   Today (Apr 29) DAY 29: PRAYER FOR THE PREPARAT...  NaN   \n",
       "29   Which is ironic given that the hordes of red r...  NaN   \n",
       "..                                                 ...  ...   \n",
       "970  During times like these, it's important to sta...  NaN   \n",
       "971  If i made u my kind of tostones u would fall i...  NaN   \n",
       "972  How to Stop Feeling So Helpless During Quarant...  NaN   \n",
       "973  Our families continue to spend quality time to...  NaN   \n",
       "974  How would you like it if someone came up to yo...  NaN   \n",
       "975                                                NaN  NaN   \n",
       "976  Bruce Cassidy shared his connection to Bobby O...  NaN   \n",
       "977  It stains my Tupperware but I think I just nee...  NaN   \n",
       "978  The Empourium to Release A Plethora Of Piñatas...  NaN   \n",
       "979  Cancer Cells Resist Mechanical Destruction in ...  NaN   \n",
       "980  Dow surges more than 400 points as positive Gi...  NaN   \n",
       "981  Dude. Is that Miss Trunchbull?! https://twitte...  NaN   \n",
       "982  Ignite #classroom-wide inventing and add a rea...  NaN   \n",
       "983  A @WSJ editorial warns: “The pandemic will pas...  NaN   \n",
       "984  Good news is that he's likely to take votes aw...  NaN   \n",
       "985  Just found out what a SWERF is. I'll save you ...  NaN   \n",
       "986                              Hard to find someone.  NaN   \n",
       "987                    Bro you keep replying let it go  NaN   \n",
       "988  And a time machine, along with the ability to ...  NaN   \n",
       "989  “If you had to get married in a chain restaura...  NaN   \n",
       "990  Lmao this is after moonie made me change it - ...  NaN   \n",
       "991  . @Cambridge_Side is looking for local artists...  NaN   \n",
       "992  Faced with a new generation of online threats,...  NaN   \n",
       "993  11 years of shouting into a void :) I was a ju...  NaN   \n",
       "994        ...... I’m tearing up for you, I really am.  NaN   \n",
       "995  Alexa B. Kimball, MD, MPH had a great conversa...  NaN   \n",
       "996                       https://youtu.be/LbXq_Zn5QFg  NaN   \n",
       "997  Follow along as Thomas Carter surprises the te...  NaN   \n",
       "998  Adams Square Baptist Church Pastor Kristopher ...  NaN   \n",
       "999  Matt and I updated our remdesivir story with m...  NaN   \n",
       "\n",
       "                              mentions                   id  \\\n",
       "0                                  NaN  1255492524556369920   \n",
       "1                                  NaN  1255492522840915968   \n",
       "2                                  NaN  1255492522723467266   \n",
       "3                                  NaN  1255492522358472707   \n",
       "4                                  NaN  1255492522144645121   \n",
       "5                                  NaN  1255492522052325387   \n",
       "6                                  NaN  1255492521884618767   \n",
       "7                            @McKinsey  1255492517518290946   \n",
       "8                                  NaN  1255492510471856132   \n",
       "9                                  NaN  1255492510346096648   \n",
       "10                                 NaN  1255492508668301316   \n",
       "11                                 NaN  1255492508462825472   \n",
       "12                                 NaN  1255492508324442114   \n",
       "13                                 NaN  1255492506931822592   \n",
       "14   @CenterForBioDiv @CenterForBioDiv  1255492505015115777   \n",
       "15                                 NaN  1255492500187463680   \n",
       "16                                 NaN  1255492489559052291   \n",
       "17                                 NaN  1255492488380514306   \n",
       "18                                 NaN  1255492484697899010   \n",
       "19                                 NaN  1255492483959701505   \n",
       "20                                 NaN  1255492482617442307   \n",
       "21                                 NaN  1255492479543017477   \n",
       "22                                 NaN  1255492479526342656   \n",
       "23                                 NaN  1255492478062362624   \n",
       "24                                 NaN  1255492476821024783   \n",
       "25                                 NaN  1255492475193622539   \n",
       "26                                 NaN  1255492474300137472   \n",
       "27                                 NaN  1255492472454688771   \n",
       "28                                 NaN  1255492472207282176   \n",
       "29                                 NaN  1255492467832602625   \n",
       "..                                 ...                  ...   \n",
       "970                                NaN  1255490806288461825   \n",
       "971                                NaN  1255490804870713349   \n",
       "972                                NaN  1255490803834773509   \n",
       "973                                NaN  1255490803117555713   \n",
       "974                                NaN  1255490796066930688   \n",
       "975                                NaN  1255490794506661890   \n",
       "976                                NaN  1255490791717437443   \n",
       "977                                NaN  1255490789968404480   \n",
       "978                                NaN  1255490789465100303   \n",
       "979                       @CellReports  1255490788194234370   \n",
       "980                         @CNBChttps  1255490787573456906   \n",
       "981                                NaN  1255490786956914688   \n",
       "982                                NaN  1255490786751365125   \n",
       "983                               @WSJ  1255490786403266560   \n",
       "984                                NaN  1255490785732108288   \n",
       "985                                NaN  1255490785556008962   \n",
       "986                                NaN  1255490784855539713   \n",
       "987                                NaN  1255490783832129540   \n",
       "988                                NaN  1255490781906903041   \n",
       "989                                NaN  1255490780338311170   \n",
       "990                                NaN  1255490779650437125   \n",
       "991                    @Cambridge_Side  1255490778933190657   \n",
       "992                   @DeepInstinctSec  1255490777628753921   \n",
       "993                                NaN  1255490777628708866   \n",
       "994                                NaN  1255490772952023041   \n",
       "995                     @Matt_R_Fisher  1255490772507525123   \n",
       "996                                NaN  1255490771500912654   \n",
       "997                                NaN  1255490771064676353   \n",
       "998                                NaN  1255490770578137090   \n",
       "999                                NaN  1255490770100015109   \n",
       "\n",
       "                                         cleaned_tweet  \\\n",
       "0    oh yes they are so bad we need left wing media...   \n",
       "1    hed be a real improvement though actual princi...   \n",
       "2    during coronavirus pandemic some massachusetts...   \n",
       "3                                         i need these   \n",
       "4         httpstwittercomwsjstatus1255492186604556290    \n",
       "5                                      keep hope alive   \n",
       "6    looks like you have to order essentials in dou...   \n",
       "7    revisiting agile teams after an abrupt shift t...   \n",
       "8    he got strapped by the mavs defense they playe...   \n",
       "9       is there a fee difference for flaccid vs erect   \n",
       "10                                                       \n",
       "11   having a great morning ive listened to pups ne...   \n",
       "12                                 im tired of smoking   \n",
       "13   the cribbo animalcrossing acnh nintendoswitchp...   \n",
       "14   apparently centerforbiodiv has a population an...   \n",
       "15                               ohhh uh uh what he do   \n",
       "16   this tweet is in response to dems falling all ...   \n",
       "17   a mere gut feeling that tether is receiving bi...   \n",
       "18              tell that nigga kid smoove to sit down   \n",
       "19   httpstwittercomhirosetakaostatus12554862668351...   \n",
       "20   what has been bothering me this entire time is...   \n",
       "21                                                   b   \n",
       "22                         just another day to improve   \n",
       "23   engine 42 respiratory distress 71 heath street...   \n",
       "24   just so hollow and ineffectual for the most pa...   \n",
       "25   hi salisbury friends i dont get on this accoun...   \n",
       "26   shouldnt he be fined for not wearing a maskhtt...   \n",
       "27   covid symptom tracker  help slow the spread of...   \n",
       "28   today apr 29 day 29 prayer for the preparation...   \n",
       "29   which is ironic given that the hordes of red r...   \n",
       "..                                                 ...   \n",
       "970  during times like these its important to stay ...   \n",
       "971  if i made u my kind of tostones u would fall i...   \n",
       "972  how to stop feeling so helpless during quarant...   \n",
       "973  our families continue to spend quality time to...   \n",
       "974  how would you like it if someone came up to yo...   \n",
       "975                                                      \n",
       "976  bruce cassidy shared his connection to bobby o...   \n",
       "977  it stains my tupperware but i think i just nee...   \n",
       "978  the empourium to release a plethora of piatas ...   \n",
       "979  cancer cells resist mechanical destruction in ...   \n",
       "980  dow surges more than 400 points as positive gi...   \n",
       "981  dude is that miss trunchbull httpstwittercomha...   \n",
       "982  ignite classroomwide inventing and add a realw...   \n",
       "983  a wsj editorial warns the pandemic will pass b...   \n",
       "984  good news is that hes likely to take votes awa...   \n",
       "985  just found out what a swerf is ill save you th...   \n",
       "986                               hard to find someone   \n",
       "987                    bro you keep replying let it go   \n",
       "988  and a time machine along with the ability to s...   \n",
       "989  if you had to get married in a chain restauran...   \n",
       "990  lmao this is after moonie made me change it  i...   \n",
       "991  cambridgeside is looking for local artists to ...   \n",
       "992  faced with a new generation of online threats ...   \n",
       "993  11 years of shouting into a void  i was a juni...   \n",
       "994                  im tearing up for you i really am   \n",
       "995  alexa b kimball md mph had a great conversatio...   \n",
       "996                                                      \n",
       "997  follow along as thomas carter surprises the te...   \n",
       "998  adams square baptist church pastor kristopher ...   \n",
       "999  matt and i updated our remdesivir story with m...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [oh, yes, bad, need, left, wing, medium, know,...   \n",
       "1    [hed, real, improvement, though, actual, princ...   \n",
       "2    [coronavirus, pandemic, massachusetts, high, s...   \n",
       "3                                               [need]   \n",
       "4    [httpstwittercomwsjstatus, 12554921866, 04556290]   \n",
       "5                                  [keep, hope, alive]   \n",
       "6    [look, like, order, essential, double, one, ar...   \n",
       "7    [revisiting, agile, team, abrupt, shift, remot...   \n",
       "8    [got, strapped, mass, defense, played, zone, l...   \n",
       "9                 [fee, difference, flaccid, v, erect]   \n",
       "10                                                  []   \n",
       "11   [great, morning, ive, listened, pup, new, song...   \n",
       "12                                [im, tired, smoking]   \n",
       "13   [cribo, animalcrossing, acne, nintendoswitchpi...   \n",
       "14   [apparently, centerforbiodiv, population, sust...   \n",
       "15                                      [oath, uh, uh]   \n",
       "16   [tweet, response, gem, falling, scold, pilosis...   \n",
       "17   [mere, gut, feeling, tether, receiving, billio...   \n",
       "18                     [tell, nigua, kid, snoove, sit]   \n",
       "19   [httpstwittercomhirosetakaostatus, 12554862668...   \n",
       "20   [bothering, entire, time, quidproquo, actually...   \n",
       "21                                                 [b]   \n",
       "22                             [another, day, improve]   \n",
       "23   [engine, 42, respiratory, distress, 71, heath,...   \n",
       "24   [hollow, ineffectual, part, ordinary, conversa...   \n",
       "25   [hi, salisbury, friend, dont, get, account, mu...   \n",
       "26   [shouldnt, fined, wearing, maskhttpstwittercom...   \n",
       "27   [coronavirus, symptom, tracker, help, slow, sp...   \n",
       "28   [today, apr, 29, day, 29, prayer, preparation,...   \n",
       "29   [ironic, given, horde, red, rose, democratic, ...   \n",
       "..                                                 ...   \n",
       "970  [time, like, important, stay, connected, frien...   \n",
       "971  [made, you, kind, topstones, you, would, fall,...   \n",
       "972  [stop, feeling, helpless, quarantinehttpsgreat...   \n",
       "973  [family, continue, spend, quality, time, toget...   \n",
       "974  [would, like, someone, came, told, arent, vali...   \n",
       "975                                                 []   \n",
       "976  [bruce, cassady, shared, connection, bobby, ou...   \n",
       "977  [stain, supperward, think, need, get, get, bac...   \n",
       "978  [emporium, release, plethora, riata, mexican, ...   \n",
       "979  [cancer, cell, resist, mechanical, destruction...   \n",
       "980  [dow, surge, 400, point, positive, glead, news...   \n",
       "981  [dude, miss, trunchbull, httpstwittercomharrys...   \n",
       "982  [ignite, classroomwide, inventing, add, dreamw...   \n",
       "983  [wsw, editorial, warns, pandemic, pas, used, c...   \n",
       "984  [good, news, he, likely, take, vote, away, trump]   \n",
       "985  [found, swerf, ill, save, time, woman, basic, ...   \n",
       "986                              [hard, find, someone]   \n",
       "987                     [bro, keep, replying, let, go]   \n",
       "988  [time, machine, along, ability, seduce, succes...   \n",
       "989  [get, married, chain, restaurant, one, would, ...   \n",
       "990  [laughing my ass off, moonie, made, change, on...   \n",
       "991  [cambridgeside, looking, local, artist, featur...   \n",
       "992  [faced, new, generation, online, threat, smart...   \n",
       "993  [11, year, shouting, void, junior, high, schoo...   \n",
       "994                              [im, tearing, really]   \n",
       "995  [alexia, b, simball, md, mph, great, conversat...   \n",
       "996                                                 []   \n",
       "997  [follow, along, thomas, carter, surprise, team...   \n",
       "998  [adam, square, baptist, church, pastor, christ...   \n",
       "999  [matt, updated, remdesivir, story, information...   \n",
       "\n",
       "                                       parts_of_speech chunks entities  \n",
       "0    [(oh, UH), (yes, RB), (bad, JJ), (need, NN), (...                  \n",
       "1    [(hed, VBN), (real, JJ), (improvement, NN), (t...                  \n",
       "2    [(coronavirus, NN), (pandemic, JJ), (massachus...                  \n",
       "3                                         [(need, NN)]                  \n",
       "4    [(httpstwittercomwsjstatus, NN), (12554921866,...                  \n",
       "5                [(keep, VB), (hope, NN), (alive, JJ)]                  \n",
       "6    [(look, NN), (like, IN), (order, NN), (essenti...                  \n",
       "7    [(revisiting, VBG), (agile, JJ), (team, NN), (...                  \n",
       "8    [(got, VBD), (strapped, JJ), (mass, NN), (defe...                  \n",
       "9    [(fee, NN), (difference, NN), (flaccid, NN), (...                  \n",
       "10                                                  []                  \n",
       "11   [(great, JJ), (morning, NN), (ive, NN), (liste...                  \n",
       "12             [(im, NN), (tired, VBD), (smoking, NN)]                  \n",
       "13   [(cribo, NN), (animalcrossing, VBG), (acne, JJ...                  \n",
       "14   [(apparently, RB), (centerforbiodiv, JJ), (pop...                  \n",
       "15                    [(oath, NN), (uh, NN), (uh, NN)]                  \n",
       "16   [(tweet, NN), (response, NN), (gem, NN), (fall...                  \n",
       "17   [(mere, RB), (gut, JJ), (feeling, VBG), (tethe...                  \n",
       "18   [(tell, VB), (nigua, JJ), (kid, NN), (snoove, ...                  \n",
       "19   [(httpstwittercomhirosetakaostatus, NN), (1255...                  \n",
       "20   [(bothering, VBG), (entire, JJ), (time, NN), (...                  \n",
       "21                                           [(b, NN)]                  \n",
       "22           [(another, DT), (day, NN), (improve, VB)]                  \n",
       "23   [(engine, NN), (42, CD), (respiratory, NN), (d...                  \n",
       "24   [(hollow, JJ), (ineffectual, JJ), (part, NN), ...                  \n",
       "25   [(hi, JJ), (salisbury, NN), (friend, NN), (don...                  \n",
       "26   [(shouldnt, NN), (fined, VBD), (wearing, VBG),...                  \n",
       "27   [(coronavirus, NN), (symptom, NN), (tracker, N...                  \n",
       "28   [(today, NN), (apr, VBD), (29, CD), (day, NN),...                  \n",
       "29   [(ironic, JJ), (given, VBN), (horde, NN), (red...                  \n",
       "..                                                 ...    ...      ...  \n",
       "970  [(time, NN), (like, IN), (important, JJ), (sta...                  \n",
       "971  [(made, VBN), (you, PRP), (kind, NN), (topston...                  \n",
       "972  [(stop, VB), (feeling, NN), (helpless, JJ), (q...                  \n",
       "973  [(family, NN), (continue, VBP), (spend, VBP), ...                  \n",
       "974  [(would, MD), (like, VB), (someone, NN), (came...                  \n",
       "975                                                 []                  \n",
       "976  [(bruce, NN), (cassady, NN), (shared, VBN), (c...                  \n",
       "977  [(stain, RB), (supperward, JJ), (think, VBP), ...                  \n",
       "978  [(emporium, NN), (release, NN), (plethora, NN)...                  \n",
       "979  [(cancer, NN), (cell, NN), (resist, VB), (mech...                  \n",
       "980  [(dow, NN), (surge, NN), (400, CD), (point, NN...                  \n",
       "981  [(dude, NN), (miss, CC), (trunchbull, NN), (ht...                  \n",
       "982  [(ignite, JJ), (classroomwide, NN), (inventing...                  \n",
       "983  [(wsw, JJ), (editorial, NN), (warns, NNS), (pa...                  \n",
       "984  [(good, JJ), (news, NN), (he, PRP), (likely, R...                  \n",
       "985  [(found, VBN), (swerf, NN), (ill, NN), (save, ...                  \n",
       "986           [(hard, JJ), (find, VBP), (someone, NN)]                  \n",
       "987  [(bro, NN), (keep, VB), (replying, VBG), (let,...                  \n",
       "988  [(time, NN), (machine, NN), (along, IN), (abil...                  \n",
       "989  [(get, VB), (married, VBN), (chain, NN), (rest...                  \n",
       "990  [(laughing my ass off, NN), (moonie, NN), (mad...                  \n",
       "991  [(cambridgeside, NN), (looking, VBG), (local, ...                  \n",
       "992  [(faced, VBN), (new, JJ), (generation, NN), (o...                  \n",
       "993  [(11, CD), (year, NN), (shouting, VBG), (void,...                  \n",
       "994           [(im, NN), (tearing, VBG), (really, RB)]                  \n",
       "995  [(alexia, NN), (b, NN), (simball, NN), (md, NN...                  \n",
       "996                                                 []                  \n",
       "997  [(follow, VB), (along, IN), (thomas, NN), (car...                  \n",
       "998  [(adam, NNS), (square, JJ), (baptist, JJ), (ch...                  \n",
       "999  [(matt, NN), (updated, VBD), (remdesivir, JJ),...                  \n",
       "\n",
       "[1000 rows x 12 columns]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    ''' Basic steps to clean text and strip out conflicting elements '''\n",
    "    try:\n",
    "        # Convert text to lowercase\n",
    "        tweet = tweet.lower()\n",
    "        # Remove hyperlinks\n",
    "        tweet = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        tweet = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        # Remove puntucation\n",
    "        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Remove white space\n",
    "        tweet = tweet.strip()\n",
    "        # Remove non ascii characters\n",
    "        tweet = ascii_normalize(tweet)\n",
    "        # Return Finished Product\n",
    "        return tweet\n",
    "    except Exception as e:\n",
    "        return ''\n",
    "\n",
    "def spell_check(tweet_words):\n",
    "    ''' Auto correct tweets according to english language standards '''\n",
    "    # Tweets are incredibly irregular and often quickly typed\n",
    "    spell = Speller(lang='en')\n",
    "    return [spell(word) for word in tweet_words]\n",
    "\n",
    "def remove_slang(tweet_words):\n",
    "    ''' Remove the slang words from dictionary '''\n",
    "    outData = []\n",
    "    for word in tweet_words:\n",
    "        outData.append(slangDict.get(word, word))\n",
    "    return outData\n",
    "\n",
    "def ascii_normalize(tweet):\n",
    "    ''' Remove non ascii characters '''\n",
    "    encodeString = tweet.encode(\"ascii\", \"ignore\")\n",
    "    return encodeString.decode()\n",
    "    \n",
    "def tokenize_and_stop(tweet):\n",
    "    ''' Convert string to tokens and remove stop words '''\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    tokens = tweet_tokenizer.tokenize(tweet)\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    cleaned = [t for t in tokens if t not in stopWords]\n",
    "    return cleaned\n",
    "\n",
    "def lem_tokens(tokens):\n",
    "    ''' Lemmanization, extract root words '''\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def pos_tag(tokens):\n",
    "    ''' Associate part of speech with the word tokens '''\n",
    "    return nltk.pos_tag(tokens)\n",
    "    \n",
    "def chunking(posRecords):\n",
    "    ''' Apply chunking on in appropriate segements (prestep for NLP) '''\n",
    "    pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "    chunkParser = nltk.RegexpParser(pattern)\n",
    "    chunks = chunkParser.parse(posRecords)\n",
    "    # IOB = Inside, outside, beginning - great for understanding where chunk comes\n",
    "    iob_tagged = tree2conlltags(chunks)\n",
    "    return iob_tagged\n",
    "\n",
    "def named_entity_recognition_master(pos, tokens, cleanedTweet):\n",
    "    ''' Perform named entity recognition on the part of speech tagged chunks '''\n",
    "    usefulTags= ['WORK_OF_ART','DATE','LOC','PERSON','LOCATION','ORGANIZATION','GEO', 'ORG','PER', 'GPE','TIME','EVB']\n",
    "    # NTLK\n",
    "    nltkEntities = nltk.ne_chunk(pos)\n",
    "    # Spacey\n",
    "    spaceyEntities = spacey_tokenize(cleanedTweet)\n",
    "    # Stanford NLP\n",
    "    stanfordEntities = []#stanford_tokenize(tokens)\n",
    "    outData = set()\n",
    "    # Cross check that we only pull out relevant tags\n",
    "    for val in spaceyEntities:\n",
    "        if val[1] in usefulTags:\n",
    "            outData.add(val)\n",
    "    for val in stanfordEntities:\n",
    "        if val[1] in usefulTags:\n",
    "            outData.add(val)\n",
    "    for val in get_continuous_chunks(nltkEntities):\n",
    "        if val[1] in usefulTags:\n",
    "            outData.add(val)\n",
    "    return copy.deepcopy(outData)\n",
    "\n",
    "def nltK_tokenize(posTags):\n",
    "    ''' Return nltk tokens from the part of speech tags '''\n",
    "    return nltk.ne_chunk(posTags)\n",
    "    \n",
    "def spacey_tokenize(tweet):\n",
    "    ''' Return spacey named entities from the cleaned tweet '''\n",
    "    doc = nlp(tweet)\n",
    "    return [(X.text, X.label_) for X in doc.ents]\n",
    "\n",
    "def stanford_tokenize(tokens):\n",
    "    ''' Use the stanford classification libarary to parse out locations '''\n",
    "    return st.tag(tokenized_text)\n",
    "    \n",
    "\n",
    "def token_pipeline(tweet):\n",
    "    ''' Pipeline to turn cleaned tweet into tokens '''\n",
    "    tokens = tokenize_and_stop(tweet)\n",
    "    tokens = remove_slang(tokens)\n",
    "    tokens = spell_check(tokens)\n",
    "    tokens = lem_tokens(tokens)\n",
    "    return tokens\n",
    "\n",
    "def pipeline(record):\n",
    "    ''' Main pipeline to ingest, clean, and process the tweets'''\n",
    "    tweet = record.get('tweet')\n",
    "    tweet = cleanText(tweet)\n",
    "    tokens = tokenizeAndStop(tweet)\n",
    "    tokens = spell_check(tokens)\n",
    "    tokens = lemTokens(tokens)\n",
    "    pos = posTag(tokens)\n",
    "    chunks = chunking(pos)\n",
    "    names = namedEntityRecognition(pos)\n",
    "    record['parsedTweet'] = tweet\n",
    "    record['tokens'] = tokens\n",
    "    record['pos'] = pos\n",
    "    record['chunks'] = chunks\n",
    "    record['ner'] = names\n",
    "    return record\n",
    "    \n",
    "def get_continuous_chunks(chunked):\n",
    "    ''' Extract entities from NLTK tree '''\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            print('trre!')\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "            else:\n",
    "                continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date                                             2020-04-29 13:41:54\n",
      "username                                                   metamitya\n",
      "to                                                      steveschoger\n",
      "text               https://twitter.com/metamitya/status/124655593...\n",
      "geo                                                              NaN\n",
      "mentions                                                         NaN\n",
      "id                                               1255492508668301316\n",
      "cleaned_tweet                                                       \n",
      "tokens                                                            []\n",
      "parts_of_speech                                                   []\n",
      "chunks                                                              \n",
      "entities           {(Street, ORGANIZATION), (Wall, ORGANIZATION),...\n",
      "topics                                                        [0, 1]\n",
      "sentiment                                                          0\n",
      "setinment                                                          0\n",
      "Name: 10, dtype: object\n",
      "set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = data.iloc[10]\n",
    "print(row)\n",
    "print(named_entity_recognition_master(row['parts_of_speech'], row['tokens'], row['cleaned_tweet']))\n",
    "print(row['cleaned_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#         SEMANTIC ANALYSIS           #\n",
    "#######################################\n",
    "def semantic_analysis_initialization(num_topics):\n",
    "    ''' TF-IDF analyis to extract key terms from each tweet with further enrichment of LDA '''\n",
    "    dictionary_LDA = corpora.Dictionary(data['tokens'])\n",
    "    dictionary_LDA.filter_extremes(no_below=3)\n",
    "    corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in data['tokens']]\n",
    "\n",
    "    %time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                      id2word=dictionary_LDA, \\\n",
    "                                      passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                      eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "    return lda_model\n",
    "\n",
    "def apply_topics(tokens):\n",
    "    global model\n",
    "    potentialTopics = lda_model[dictionary_LDA.doc2bow(tokens)]\n",
    "    # Sort topics\n",
    "    potentialTopics = sorted(potentialTopics, key = lambda x: x[1])\n",
    "    # Extract top 3 topics\n",
    "    return [x[0] for x in potentialTopics[0:2]]\n",
    "\n",
    "#######################################\n",
    "#         SENTIMENT ANALYSIS          #\n",
    "#######################################\n",
    "def sentiment_analysis(tweet):\n",
    "    ''' Simple Sentiment Analysis - Scored from -5 to 5 (negative to positive) '''\n",
    "    afinn = Afinn()\n",
    "    return afinn.score(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "model = semantic_analysis_initialization(20)\n",
    "topics = []\n",
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    topics.append('Topics-' + str(i) + ': ' + topic)\n",
    "# Initialize empty topics for each tweet\n",
    "data['topics'] = data['tokens'].apply(apply_topics)\n",
    "data['sentiment'] = data['cleaned_tweet'].apply(sentiment_analysis)\n",
    "data['entities'] = data.apply(lambda row: named_entity_recognition_master(row['parts_of_speech'], row['tokens'], row['cleaned_tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('out_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#         GEOCODING SERVICES          #\n",
    "#######################################\n",
    "\n",
    "def googleEnrichments(entity):\n",
    "    ''' Extract named entities and perform a location using google geolocation servies '''\n",
    "    api_key = 'AIzaSyBc-nbD2tOiOaNHRLButAEScw3fXcuL4Tw'\n",
    "    base_url = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json?'\n",
    "    # Now we need to define the parameters\n",
    "    returnData = 'fields=formatted_address,name,geometry,types'\n",
    "    url = base_url + 'key=' + api_key + '&' + 'inputtype=textquery&' + 'input=' + entity + '&' + returnData\n",
    "    response = requests.get(url)\n",
    "    return cleanGoogleResponse(response)\n",
    "\n",
    "def cleanGoogleResponse(response):\n",
    "    ''' Ingest response from Google API and extract the useable fields from it '''\n",
    "    if response.status_code == 200:\n",
    "        # Successful Request\n",
    "        loadedData = json.loads(response.text)\n",
    "        # Create Parsed location Data\n",
    "        candidates = loadedData.get('candidates', [])\n",
    "        # Remove viewport from the candidates array\n",
    "        for x in range(len(candidates)):\n",
    "            latLon = candidates[x]['geometry']['location']\n",
    "            del candidates[x]['geometry']\n",
    "            candidates[x]['location'] = latLon\n",
    "        # Return the candidates array\n",
    "        return candidates\n",
    "    else:\n",
    "        # Bad Request\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'formatted_address': '5 Forbes Rd, Lexington, MA 02421, United States',\n",
       "  'name': 'MIT Lincoln Laboratory',\n",
       "  'types': ['point_of_interest', 'establishment'],\n",
       "  'location': {'lat': 42.4430327, 'lng': -71.2627716}},\n",
       " {'formatted_address': '244 Wood St, Lexington, MA 02421, United States',\n",
       "  'name': 'Massachusetts Institute of Technology: Lincoln Laboratory',\n",
       "  'types': ['point_of_interest', 'establishment'],\n",
       "  'location': {'lat': 42.4591187, 'lng': -71.26704459999999}}]"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = googleEnrichments('MIT LL')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#         Deep Neural Network         #\n",
    "#######################################\n",
    "from keras.layers import  Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def read_corpus_nn():\n",
    "    ''' Read in downloaded tweet meta data and extract key fields '''\n",
    "    data = pd.read_csv(\"labeled_data.csv\")\n",
    "    return data\n",
    "\n",
    "def TFIDF(X_train, X_test,MAX_NB_WORDS=75000):\n",
    "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "    print(\"tf-idf with\",str(np.array(X_train).shape[1]),\"features\")\n",
    "    return (X_train,X_test)\n",
    "\n",
    "def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):\n",
    "    \"\"\"\n",
    "    buildModel_DNN_Tex(shape, nClasses,dropout)\n",
    "    Build Deep neural networks Model for text classification\n",
    "    Shape is input feature space\n",
    "    nClasses is number of classes\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    nLayers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,nLayers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 3070 features\n",
      "Train on 600 samples, validate on 100 samples\n",
      "Epoch 1/25\n",
      " - 0s - loss: 1.4718 - accuracy: 0.5167 - val_loss: 1.2679 - val_accuracy: 0.5900\n",
      "Epoch 2/25\n",
      " - 0s - loss: 1.2336 - accuracy: 0.6067 - val_loss: 1.2554 - val_accuracy: 0.5900\n",
      "Epoch 3/25\n",
      " - 0s - loss: 1.1071 - accuracy: 0.6067 - val_loss: 1.2675 - val_accuracy: 0.5900\n",
      "Epoch 4/25\n",
      " - 0s - loss: 1.0343 - accuracy: 0.6067 - val_loss: 1.2513 - val_accuracy: 0.5900\n",
      "Epoch 5/25\n",
      " - 0s - loss: 0.9711 - accuracy: 0.6067 - val_loss: 1.1994 - val_accuracy: 0.5900\n",
      "Epoch 6/25\n",
      " - 0s - loss: 0.8790 - accuracy: 0.6067 - val_loss: 1.1768 - val_accuracy: 0.5900\n",
      "Epoch 7/25\n",
      " - 0s - loss: 0.7709 - accuracy: 0.6067 - val_loss: 1.2166 - val_accuracy: 0.5900\n",
      "Epoch 8/25\n",
      " - 0s - loss: 0.6958 - accuracy: 0.6067 - val_loss: 1.2491 - val_accuracy: 0.5700\n",
      "Epoch 9/25\n",
      " - 0s - loss: 0.6575 - accuracy: 0.6650 - val_loss: 1.2918 - val_accuracy: 0.5200\n",
      "Epoch 10/25\n",
      " - 0s - loss: 0.6439 - accuracy: 0.7433 - val_loss: 1.4476 - val_accuracy: 0.5500\n",
      "Epoch 11/25\n",
      " - 0s - loss: 0.6246 - accuracy: 0.7517 - val_loss: 1.6182 - val_accuracy: 0.5500\n",
      "Epoch 12/25\n",
      " - 0s - loss: 0.5986 - accuracy: 0.7350 - val_loss: 1.7264 - val_accuracy: 0.5500\n",
      "Epoch 13/25\n",
      " - 0s - loss: 0.5701 - accuracy: 0.7533 - val_loss: 2.0460 - val_accuracy: 0.5500\n",
      "Epoch 14/25\n",
      " - 0s - loss: 0.5358 - accuracy: 0.7433 - val_loss: 2.6884 - val_accuracy: 0.5300\n",
      "Epoch 15/25\n",
      " - 0s - loss: 0.5089 - accuracy: 0.7467 - val_loss: 2.4484 - val_accuracy: 0.5100\n",
      "Epoch 16/25\n",
      " - 0s - loss: 0.4578 - accuracy: 0.7650 - val_loss: 2.4837 - val_accuracy: 0.4100\n",
      "Epoch 17/25\n",
      " - 0s - loss: 0.4336 - accuracy: 0.7767 - val_loss: 3.3139 - val_accuracy: 0.4700\n",
      "Epoch 18/25\n",
      " - 0s - loss: 0.3883 - accuracy: 0.8267 - val_loss: 3.3909 - val_accuracy: 0.4600\n",
      "Epoch 19/25\n",
      " - 0s - loss: 0.3579 - accuracy: 0.8800 - val_loss: 3.4693 - val_accuracy: 0.4700\n",
      "Epoch 20/25\n",
      " - 0s - loss: 0.3365 - accuracy: 0.8917 - val_loss: 3.7241 - val_accuracy: 0.4700\n",
      "Epoch 21/25\n",
      " - 0s - loss: 0.3037 - accuracy: 0.8883 - val_loss: 3.9791 - val_accuracy: 0.5200\n",
      "Epoch 22/25\n",
      " - 0s - loss: 0.2734 - accuracy: 0.8867 - val_loss: 4.2693 - val_accuracy: 0.4800\n",
      "Epoch 23/25\n",
      " - 0s - loss: 0.2529 - accuracy: 0.8850 - val_loss: 3.9708 - val_accuracy: 0.5300\n",
      "Epoch 24/25\n",
      " - 0s - loss: 0.2425 - accuracy: 0.8850 - val_loss: 4.2322 - val_accuracy: 0.4200\n",
      "Epoch 25/25\n",
      " - 0s - loss: 0.2182 - accuracy: 0.8967 - val_loss: 4.6798 - val_accuracy: 0.5300\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77        59\n",
      "           1       0.08      0.18      0.11        11\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.33      0.25      0.29        12\n",
      "\n",
      "    accuracy                           0.53       100\n",
      "   macro avg       0.23      0.25      0.23       100\n",
      "weighted avg       0.48      0.53      0.50       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Deep Learning Execution ###\n",
    "data_nn = read_corpus_nn()\n",
    "# Extract only relevant information\n",
    "data_nn = data_nn[['text', 'health rating']]\n",
    "# Subset Data\n",
    "data_nn = data_nn.iloc[:700]\n",
    "# Fill any missing values\n",
    "data_nn = data_nn.fillna(0)\n",
    "# Run pipeline to generate additional input features\n",
    "data_nn['cleaned_tweet'] = data_nn['text'].apply(clean_text)\n",
    "# Run sentiment analysis of data\n",
    "data_nn['sentiment'] = data_nn['cleaned_tweet'].apply(sentiment_analysis)\n",
    "# Model split\n",
    "X_train = data_nn['cleaned_tweet'].iloc[:600].astype(str)\n",
    "X_test =  data_nn['cleaned_tweet'].iloc[600:].astype(str)\n",
    "Y_train = data_nn['health rating'].iloc[:600].astype(int)\n",
    "Y_test =  data_nn['health rating'].iloc[600:].astype(int)\n",
    "X_train_tfidf,X_test_tfidf = TFIDF(X_train,X_test)\n",
    "# Zip tfidf with the sentiment again\n",
    "X_train = list(zip(X_train_tfidf, data_nn['health rating'].iloc[600:]))\n",
    "X_test = list(zip(X_test_tfidf, data_nn['health rating'].iloc[600:]))\n",
    "model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 5)\n",
    "model_DNN.fit(X_train_tfidf, Y_train,\n",
    "                              validation_data=(X_test_tfidf, Y_test),\n",
    "                              epochs=25,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "predicted = model_DNN.predict_classes(X_test_tfidf)\n",
    "print(metrics.classification_report(Y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 4, 0, 0, 0, 1, 0, 0, 1, 4, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 4, 1, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0,\n",
       "       0, 0, 4, 1, 0, 0, 0, 0, 4, 4, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      1.0\n",
       "2      1.0\n",
       "3      0.0\n",
       "4      3.0\n",
       "5      3.0\n",
       "6      0.0\n",
       "7      0.0\n",
       "8      0.0\n",
       "9      0.0\n",
       "10     0.0\n",
       "11     0.0\n",
       "12     4.0\n",
       "13     0.0\n",
       "14     1.0\n",
       "15     3.0\n",
       "16     0.0\n",
       "17     1.0\n",
       "18     0.0\n",
       "19     0.0\n",
       "20     0.0\n",
       "21     0.0\n",
       "22     4.0\n",
       "23     0.0\n",
       "24     0.0\n",
       "25     4.0\n",
       "26     0.0\n",
       "27     0.0\n",
       "28     1.0\n",
       "29     1.0\n",
       "      ... \n",
       "670    2.0\n",
       "671    3.0\n",
       "672    0.0\n",
       "673    4.0\n",
       "674    2.0\n",
       "675    0.0\n",
       "676    0.0\n",
       "677    0.0\n",
       "678    2.0\n",
       "679    0.0\n",
       "680    4.0\n",
       "681    0.0\n",
       "682    0.0\n",
       "683    0.0\n",
       "684    1.0\n",
       "685    0.0\n",
       "686    1.0\n",
       "687    0.0\n",
       "688    0.0\n",
       "689    0.0\n",
       "690    0.0\n",
       "691    4.0\n",
       "692    2.0\n",
       "693    0.0\n",
       "694    4.0\n",
       "695    0.0\n",
       "696    2.0\n",
       "697    4.0\n",
       "698    2.0\n",
       "699    0.0\n",
       "Name: health rating, Length: 700, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nn['health rating'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
