{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnltk.download(\\'punkt\\') -> missing tokenzer, might need installing\\nfor dependency in (\"words\", brown\", \"names\", \"wordnet\", \"averaged_perceptron_tagger\", \"universal_tagset\", \"stop_words\", \"maxent_ne_chunker\", \"punkt\"):\\n    nltk.download(dependency)\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Statements\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import gzip\n",
    "import copy\n",
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "from afinn import Afinn\n",
    "from pathlib import Path\n",
    "from spacy import displacy\n",
    "from normalise import normalise\n",
    "from normalise import tokenize_basic\n",
    "from autocorrect import Speller\n",
    "from collections import Counter\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "### LOAD CONFIGURATION LIBARIES\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_231/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "nlp = en_core_web_sm.load()\n",
    "st = StanfordNERTagger('C:/Users/NA29187/Downloads/stanford-ner-2018-10-16/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   'C:/Users/NA29187/Downloads/stanford-ner-2018-10-16/stanford-ner-2018-10-16/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\"\"\"\n",
    "nltk.download('punkt') -> missing tokenzer, might need installing\n",
    "for dependency in (\"words\", brown\", \"names\", \"wordnet\", \"averaged_perceptron_tagger\", \"universal_tagset\", \"stop_words\", \"maxent_ne_chunker\", \"punkt\"):\n",
    "    nltk.download(dependency)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangDict = {\n",
    "    'afaik': 'as far as i know',\n",
    "    'bc': 'because',\n",
    "    'bfn': 'bye for now',\n",
    "    'br': 'best regards',\n",
    "    'btw': 'by the way',\n",
    "    'dm': 'direct message',\n",
    "    'em': 'email',\n",
    "    'fb': 'facebook',\n",
    "    'ffs': 'for fucks sake',\n",
    "    'fml': 'fuck my life',\n",
    "    'rt': 'retweet',\n",
    "    'bgd': 'background',\n",
    "    'ab': 'about',\n",
    "    'abt': 'about',\n",
    "    'dd': 'dear daughter',\n",
    "    'ayfkmwts': 'are you fucking kidding me with this shit',\n",
    "    'w': 'with',\n",
    "    'wo': 'without',\n",
    "    'dyk': 'did you know',\n",
    "    'eli5': 'explain it to me like i am five',\n",
    "    'fbf': 'flashback friday',\n",
    "    'fomo': 'fear of missing out',\n",
    "    'ftw': 'for the win',\n",
    "    'fyi': 'for your information',\n",
    "    'icymi': 'in case you missed it',\n",
    "    'ht': 'hat tip',\n",
    "    'imo': 'in my opinion',\n",
    "    'imho': 'in my humble opinion',\n",
    "    'irl': 'in real life',\n",
    "    'lmk': 'let me know',\n",
    "    'nbd': 'no big deal',\n",
    "    'nsfw': 'not safe for work',\n",
    "    'wfh': 'working from home',\n",
    "    'covid': 'coronavirus',\n",
    "    'smh': 'shaking my head',\n",
    "    'tbh': 'to be honest',\n",
    "    'tbt': 'throwback thursday',\n",
    "    'tfw': 'that feeling when',\n",
    "    'tgif': 'thank god it is friday',\n",
    "    'tldr': 'too long did not read',\n",
    "    'wbw': 'way back wednesday',\n",
    "    'af': 'as fuck',\n",
    "    'bae': 'my crush',\n",
    "    'hmu': 'hit me up',\n",
    "    'idk': 'i do not know',\n",
    "    'idc': 'i do not care',\n",
    "    'fwiw': 'for what it is worth',\n",
    "    'ily': 'i love you',\n",
    "    'iso': 'in search of',\n",
    "    'jk': 'just kidding',\n",
    "    'jtm': 'just the messenger',\n",
    "    'lmao': 'laughing my ass off',\n",
    "    'lol': 'laughing out loud',\n",
    "    'rip': 'rest in peace',\n",
    "    'nvm': 'nevermind',\n",
    "    'obv': 'obviously',\n",
    "    'omg': 'oh my god',\n",
    "    'omw': 'on my way',\n",
    "    'pls': 'please',\n",
    "    'psa': 'public service announcement',\n",
    "    'rn': 'right now',\n",
    "    'rofl': 'rolling on the floor laughing',\n",
    "    'srsly': 'seriously',\n",
    "    'til': 'today i learned',\n",
    "    'tmi': 'too much information',\n",
    "    'ty': 'thank you',\n",
    "    'wtf': 'what the fuck',\n",
    "    'yw': 'you are welcome',\n",
    "    'bfn': 'bye for now',\n",
    "    'da': 'the',\n",
    "    'deets': 'details',\n",
    "    'fab': 'fabulous',\n",
    "    'fav': 'favorite',\n",
    "    'ic': 'i see',\n",
    "    'kk': 'cool cool',\n",
    "    'nts': 'note to self',\n",
    "    'prt': 'please retweet',\n",
    "    'tftf': 'thank for the follow',\n",
    "    'tmb': 'tweet me back',\n",
    "    'u': 'you',\n",
    "    'woz': 'was',\n",
    "    'wtv': 'whatever',\n",
    "    'ykyat': 'you know you are addicted to',\n",
    "    'yolo': 'you only live once',\n",
    "    'yoyo': 'you are on your own'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(maxNum=1000):\n",
    "    ''' Utilize the get old tweets package to aggregate historical tweets - easier to use than Twitter API '''\n",
    "    # Build the OS call to generate the csv\n",
    "    FILE_PATH = 'C:/Users/NA29187/Downloads/Optimized-Modified-GetOldTweets3-OMGOT-master/Optimized-Modified-GetOldTweets3-OMGOT-master/GetOldTweets3-0.0.10/'\n",
    "    SCRIPT = 'GetOldTweets3.py '\n",
    "    PYTHON_PATH = 'C:/ProgramData/Anaconda3/python.exe '\n",
    "    QUERY = '--near \"42.376, -71.1097\" --within 40km --maxtweets {}'.format(maxNum)\n",
    "    print('Executing Query: {}'.format(PYTHON_PATH + FILE_PATH + SCRIPT + QUERY))\n",
    "    os.system(PYTHON_PATH + FILE_PATH + SCRIPT + QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    ''' Read in downloaded tweet meta data and extract key fields '''\n",
    "    data = pd.read_csv(\"output_got.csv\")\n",
    "    # Remove useless columns to save space in memory\n",
    "    del data['replies']\n",
    "    del data['retweets']\n",
    "    del data['favorites']\n",
    "    del data['hashtags']\n",
    "    del data['permalink']\n",
    "    # Create area to store cleaned tweet\n",
    "    data['cleaned_tweet'] = data['text']\n",
    "    data['tokens'] = ''\n",
    "    data['parts_of_speech'] = ''\n",
    "    data['chunks'] = ''\n",
    "    data['entities'] = ''\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    ''' Basic steps to clean text and strip out conflicting elements '''\n",
    "    try:\n",
    "        # Convert text to lowercase\n",
    "        tweet = tweet.lower()\n",
    "        # Remove hyperlinks\n",
    "        tweet = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        tweet = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        # Remove puntucation\n",
    "        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Remove white space\n",
    "        tweet = tweet.strip()\n",
    "        # Remove non ascii characters\n",
    "        tweet = ascii_normalize(tweet)\n",
    "        # Return Finished Product\n",
    "        return tweet\n",
    "    except Exception as e:\n",
    "        return ''\n",
    "\n",
    "def spell_check(tweet_words):\n",
    "    ''' Auto correct tweets according to english language standards '''\n",
    "    # Tweets are incredibly irregular and often quickly typed\n",
    "    spell = Speller(lang='en')\n",
    "    return [spell(word) for word in tweet_words]\n",
    "\n",
    "def remove_slang(tweet_words):\n",
    "    ''' Remove the slang words from dictionary '''\n",
    "    outData = []\n",
    "    for word in tweet_words:\n",
    "        outData.append(slangDict.get(word, word))\n",
    "    return outData\n",
    "\n",
    "def ascii_normalize(tweet):\n",
    "    ''' Remove non ascii characters '''\n",
    "    encodeString = tweet.encode(\"ascii\", \"ignore\")\n",
    "    return encodeString.decode()\n",
    "    \n",
    "def tokenize_and_stop(tweet):\n",
    "    ''' Convert string to tokens and remove stop words '''\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    tokens = tweet_tokenizer.tokenize(tweet)\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    cleaned = [t for t in tokens if t not in stopWords]\n",
    "    return cleaned\n",
    "\n",
    "def lem_tokens(tokens):\n",
    "    ''' Lemmanization, extract root words '''\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def pos_tag(tokens):\n",
    "    ''' Associate part of speech with the word tokens '''\n",
    "    return nltk.pos_tag(tokens)\n",
    "    \n",
    "def chunking(posRecords):\n",
    "    ''' Apply chunking on in appropriate segements (prestep for NLP) '''\n",
    "    pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "    chunkParser = nltk.RegexpParser(pattern)\n",
    "    chunks = chunkParser.parse(posRecords)\n",
    "    # IOB = Inside, outside, beginning - great for understanding where chunk comes\n",
    "    iob_tagged = tree2conlltags(chunks)\n",
    "    return iob_tagged\n",
    "\n",
    "def named_entity_recognition_master(pos, tokens, cleanedTweet):\n",
    "    ''' Perform named entity recognition on the part of speech tagged chunks '''\n",
    "    usefulTags= ['WORK_OF_ART','DATE','LOC','PERSON','LOCATION','ORGANIZATION','GEO', 'ORG','PER', 'GPE','TIME','EVB']\n",
    "    # NTLK\n",
    "    nltkEntities = nltk.ne_chunk(pos)\n",
    "    # Spacey\n",
    "    spaceyEntities = spacey_tokenize(cleanedTweet)\n",
    "    # Stanford NLP\n",
    "    stanfordEntities = []#stanford_tokenize(tokens)\n",
    "    outData = set()\n",
    "    # Cross check that we only pull out relevant tags\n",
    "    for val in spaceyEntities:\n",
    "        if val[1] in usefulTags:\n",
    "            outData.add(val)\n",
    "    for val in stanfordEntities:\n",
    "        if val[1] in usefulTags:\n",
    "            outData.add(val)\n",
    "    for val in get_continuous_chunks(nltkEntities):\n",
    "        if val[1] in usefulTags:\n",
    "            outData.add(val)\n",
    "    return copy.deepcopy(outData)\n",
    "\n",
    "def nltK_tokenize(posTags):\n",
    "    ''' Return nltk tokens from the part of speech tags '''\n",
    "    return nltk.ne_chunk(posTags)\n",
    "    \n",
    "def spacey_tokenize(tweet):\n",
    "    ''' Return spacey named entities from the cleaned tweet '''\n",
    "    doc = nlp(tweet)\n",
    "    return [(X.text, X.label_) for X in doc.ents]\n",
    "\n",
    "def stanford_tokenize(tokens):\n",
    "    ''' Use the stanford classification libarary to parse out locations '''\n",
    "    return st.tag(tokenized_text)\n",
    "    \n",
    "\n",
    "def token_pipeline(tweet):\n",
    "    ''' Pipeline to turn cleaned tweet into tokens '''\n",
    "    tokens = tokenize_and_stop(tweet)\n",
    "    tokens = remove_slang(tokens)\n",
    "    tokens = spell_check(tokens)\n",
    "    tokens = lem_tokens(tokens)\n",
    "    return tokens\n",
    "\n",
    "def pipeline(record):\n",
    "    ''' Main pipeline to ingest, clean, and process the tweets'''\n",
    "    tweet = record.get('tweet')\n",
    "    tweet = cleanText(tweet)\n",
    "    tokens = tokenizeAndStop(tweet)\n",
    "    tokens = spell_check(tokens)\n",
    "    tokens = lemTokens(tokens)\n",
    "    pos = posTag(tokens)\n",
    "    chunks = chunking(pos)\n",
    "    names = namedEntityRecognition(pos)\n",
    "    record['parsedTweet'] = tweet\n",
    "    record['tokens'] = tokens\n",
    "    record['pos'] = pos\n",
    "    record['chunks'] = chunks\n",
    "    record['ner'] = names\n",
    "    return record\n",
    "    \n",
    "def get_continuous_chunks(chunked):\n",
    "    ''' Extract entities from NLTK tree '''\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            print('trre!')\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "            else:\n",
    "                continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#         SEMANTIC ANALYSIS           #\n",
    "#######################################\n",
    "def semantic_analysis_initialization(num_topics):\n",
    "    ''' TF-IDF analyis to extract key terms from each tweet with further enrichment of LDA '''\n",
    "    dictionary_LDA = corpora.Dictionary(data['tokens'])\n",
    "    dictionary_LDA.filter_extremes(no_below=3)\n",
    "    corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in data['tokens']]\n",
    "\n",
    "    %time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                      id2word=dictionary_LDA, \\\n",
    "                                      passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                      eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "    return lda_model\n",
    "\n",
    "def apply_topics(tokens):\n",
    "    global model\n",
    "    potentialTopics = lda_model[dictionary_LDA.doc2bow(tokens)]\n",
    "    # Sort topics\n",
    "    potentialTopics = sorted(potentialTopics, key = lambda x: x[1])\n",
    "    # Extract top 3 topics\n",
    "    return [x[0] for x in potentialTopics[0:2]]\n",
    "\n",
    "#######################################\n",
    "#         SENTIMENT ANALYSIS          #\n",
    "#######################################\n",
    "def sentiment_analysis(tweet):\n",
    "    ''' Simple Sentiment Analysis - Scored from -5 to 5 (negative to positive) '''\n",
    "    afinn = Afinn()\n",
    "    return afinn.score(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#         GEOCODING SERVICES          #\n",
    "#######################################\n",
    "\n",
    "def googleEnrichments(entity):\n",
    "    ''' Extract named entities and perform a location using google geolocation servies '''\n",
    "    api_key = ''\n",
    "    base_url = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json?'\n",
    "    # Now we need to define the parameters\n",
    "    returnData = 'fields=formatted_address,name,geometry,types'\n",
    "    url = base_url + 'key=' + api_key + '&' + 'inputtype=textquery&' + 'input=' + entity + '&' + returnData\n",
    "    response = requests.get(url)\n",
    "    return cleanGoogleResponse(response)\n",
    "\n",
    "def cleanGoogleResponse(response):\n",
    "    ''' Ingest response from Google API and extract the useable fields from it '''\n",
    "    if response.status_code == 200:\n",
    "        # Successful Request\n",
    "        loadedData = json.loads(response.text)\n",
    "        # Create Parsed location Data\n",
    "        candidates = loadedData.get('candidates', [])\n",
    "        # Remove viewport from the candidates array\n",
    "        for x in range(len(candidates)):\n",
    "            latLon = candidates[x]['geometry']['location']\n",
    "            del candidates[x]['geometry']\n",
    "            candidates[x]['location'] = latLon\n",
    "        # Return the candidates array\n",
    "        return candidates\n",
    "    else:\n",
    "        # Bad Request\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#         Deep Neural Network         #\n",
    "#######################################\n",
    "from keras.layers import  Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def read_corpus_nn():\n",
    "    ''' Read in downloaded tweet meta data and extract key fields '''\n",
    "    data = pd.read_csv(\"labeled_data.csv\")\n",
    "    return data\n",
    "\n",
    "def TFIDF(X_train, X_test,MAX_NB_WORDS=75000):\n",
    "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "    print(\"tf-idf with\",str(np.array(X_train).shape[1]),\"features\")\n",
    "    return (X_train,X_test)\n",
    "\n",
    "def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):\n",
    "    \"\"\"\n",
    "    buildModel_DNN_Tex(shape, nClasses,dropout)\n",
    "    Build Deep neural networks Model for text classification\n",
    "    Shape is input feature space\n",
    "    nClasses is number of classes\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    nLayers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,nLayers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_corpus(500)\n",
    "data= read_corpus()\n",
    "data = data.iloc[:100]\n",
    "data['cleaned_tweet']= data['cleaned_tweet'].apply(clean_text)\n",
    "data['tokens'] = data['cleaned_tweet'].apply(token_pipeline)\n",
    "data['parts_of_speech'] = data['tokens'].apply(pos_tag)\n",
    "model = semantic_analysis_initialization(20)\n",
    "topics = []\n",
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    topics.append('Topics-' + str(i) + ': ' + topic)\n",
    "# Initialize empty topics for each tweet\n",
    "data['topics'] = data['tokens'].apply(apply_topics)\n",
    "data['sentiment'] = data['cleaned_tweet'].apply(sentiment_analysis)\n",
    "data['entities'] = data.apply(lambda row: named_entity_recognition_master(row['parts_of_speech'], row['tokens'], row['cleaned_tweet']), axis=1)\n",
    "data.to_csv('out_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 3070 features\n",
      "Train on 600 samples, validate on 100 samples\n",
      "Epoch 1/25\n",
      " - 0s - loss: 1.5004 - accuracy: 0.5217 - val_loss: 1.3147 - val_accuracy: 0.5900\n",
      "Epoch 2/25\n",
      " - 0s - loss: 1.2127 - accuracy: 0.6067 - val_loss: 1.2593 - val_accuracy: 0.5900\n",
      "Epoch 3/25\n",
      " - 0s - loss: 1.1138 - accuracy: 0.6067 - val_loss: 1.2799 - val_accuracy: 0.5900\n",
      "Epoch 4/25\n",
      " - 0s - loss: 1.0606 - accuracy: 0.6067 - val_loss: 1.2374 - val_accuracy: 0.5900\n",
      "Epoch 5/25\n",
      " - 0s - loss: 0.9731 - accuracy: 0.6067 - val_loss: 1.1925 - val_accuracy: 0.5900\n",
      "Epoch 6/25\n",
      " - 0s - loss: 0.8678 - accuracy: 0.6067 - val_loss: 1.1703 - val_accuracy: 0.5900\n",
      "Epoch 7/25\n",
      " - 0s - loss: 0.7737 - accuracy: 0.6067 - val_loss: 1.2042 - val_accuracy: 0.5900\n",
      "Epoch 8/25\n",
      " - 0s - loss: 0.6938 - accuracy: 0.6083 - val_loss: 1.1931 - val_accuracy: 0.5700\n",
      "Epoch 9/25\n",
      " - 0s - loss: 0.6644 - accuracy: 0.6800 - val_loss: 1.2409 - val_accuracy: 0.5600\n",
      "Epoch 10/25\n",
      " - 0s - loss: 0.6483 - accuracy: 0.7483 - val_loss: 1.3306 - val_accuracy: 0.5600\n",
      "Epoch 11/25\n",
      " - 0s - loss: 0.6196 - accuracy: 0.7500 - val_loss: 1.5343 - val_accuracy: 0.5700\n",
      "Epoch 12/25\n",
      " - 0s - loss: 0.6063 - accuracy: 0.7267 - val_loss: 1.6564 - val_accuracy: 0.5700\n",
      "Epoch 13/25\n",
      " - 0s - loss: 0.5784 - accuracy: 0.7417 - val_loss: 1.7725 - val_accuracy: 0.5400\n",
      "Epoch 14/25\n",
      " - 0s - loss: 0.5431 - accuracy: 0.7583 - val_loss: 2.1946 - val_accuracy: 0.5300\n",
      "Epoch 15/25\n",
      " - 0s - loss: 0.5030 - accuracy: 0.7600 - val_loss: 2.5915 - val_accuracy: 0.5300\n",
      "Epoch 16/25\n",
      " - 0s - loss: 0.4612 - accuracy: 0.7617 - val_loss: 2.4652 - val_accuracy: 0.4400\n",
      "Epoch 17/25\n",
      " - 0s - loss: 0.4283 - accuracy: 0.7733 - val_loss: 2.8821 - val_accuracy: 0.5100\n",
      "Epoch 18/25\n",
      " - 0s - loss: 0.4077 - accuracy: 0.7833 - val_loss: 3.0206 - val_accuracy: 0.3900\n",
      "Epoch 19/25\n",
      " - 0s - loss: 0.3745 - accuracy: 0.8517 - val_loss: 3.3747 - val_accuracy: 0.4900\n",
      "Epoch 20/25\n",
      " - 0s - loss: 0.3564 - accuracy: 0.8867 - val_loss: 3.1088 - val_accuracy: 0.4100\n",
      "Epoch 21/25\n",
      " - 0s - loss: 0.3289 - accuracy: 0.8767 - val_loss: 2.9079 - val_accuracy: 0.4400\n",
      "Epoch 22/25\n",
      " - 0s - loss: 0.3246 - accuracy: 0.9000 - val_loss: 3.1716 - val_accuracy: 0.5500\n",
      "Epoch 23/25\n",
      " - 0s - loss: 0.3061 - accuracy: 0.8850 - val_loss: 3.4467 - val_accuracy: 0.5300\n",
      "Epoch 24/25\n",
      " - 0s - loss: 0.2818 - accuracy: 0.8867 - val_loss: 3.8556 - val_accuracy: 0.5100\n",
      "Epoch 25/25\n",
      " - 0s - loss: 0.2441 - accuracy: 0.8950 - val_loss: 3.8187 - val_accuracy: 0.5500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76        59\n",
      "           1       0.12      0.27      0.17        11\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.42      0.42      0.42        12\n",
      "\n",
      "    accuracy                           0.55       100\n",
      "   macro avg       0.26      0.30      0.27       100\n",
      "weighted avg       0.50      0.55      0.52       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Deep Learning Execution ###\n",
    "data_nn = read_corpus_nn()\n",
    "# Extract only relevant information\n",
    "data_nn = data_nn[['text', 'health rating']]\n",
    "# Subset Data\n",
    "data_nn = data_nn.iloc[:700]\n",
    "# Fill any missing values\n",
    "data_nn = data_nn.fillna(0)\n",
    "# Run pipeline to generate additional input features\n",
    "data_nn['cleaned_tweet'] = data_nn['text'].apply(clean_text)\n",
    "# Run sentiment analysis of data\n",
    "data_nn['sentiment'] = data_nn['cleaned_tweet'].apply(sentiment_analysis)\n",
    "# Model split\n",
    "X_train = data_nn['cleaned_tweet'].iloc[:600].astype(str)\n",
    "X_test =  data_nn['cleaned_tweet'].iloc[600:].astype(str)\n",
    "X_test.append(pd.Series(\"Went for a run along the Charles River today. OOF. Starting to feel sick, and starting to cough :)\"))\n",
    "Y_train = data_nn['health rating'].iloc[:600].astype(int)\n",
    "Y_test =  data_nn['health rating'].iloc[600:].astype(int)\n",
    "X_train_tfidf,X_test_tfidf = TFIDF(X_train,X_test)\n",
    "# Zip tfidf with the sentiment again\n",
    "X_train = list(zip(X_train_tfidf, data_nn['health rating'].iloc[600:]))\n",
    "X_test = list(zip(X_test_tfidf, data_nn['health rating'].iloc[600:]))\n",
    "model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 5)\n",
    "model_DNN.fit(X_train_tfidf, Y_train,\n",
    "                              validation_data=(X_test_tfidf, Y_test),\n",
    "                              epochs=25,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "predicted = model_DNN.predict_classes(X_test_tfidf)\n",
    "print(metrics.classification_report(Y_test, predicted))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
